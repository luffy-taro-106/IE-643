# Image-to-Audio Generation using Contrastive Image-Audio Pretraining (CIAP)

A project that extends AudioLDM's text-to-audio generation capability to image-to-audio generation by replacing the CLAP (Contrastive Language-Audio Pretraining) model with a custom CIAP (Contrastive Image-Audio Pretraining) model that learns to map images and audio into a shared latent space.

## ğŸ“‹ Project Overview

### Objective
Transform a pretrained text-to-audio generation model (AudioLDM) into an image-to-audio generation model by developing a contrastive learning framework that aligns images and audio in a shared embedding space.

### Key Constraints
- âœ… **Baseline Network (AudioLDM) remains frozen** - No training or fine-tuning allowed on the diffusion model
- âœ… **Only CIAP components are trainable** - Image and audio encoders are trained from scratch
- âœ… **Uses AudioLDM as instructed baseline** - Leverages the provided pretrained diffusion model

## ğŸ—ï¸ Architecture

### System Overview

```
Input Image â†’ Image Encoder â†’ Image Embedding (512-d)
                                      â†“
                              [Contrastive Alignment]
                                      â†“
AudioLDM Diffusion Model â† Audio Embedding (512-d) â† Audio Encoder
                                      â†“
                              Generated Audio
```

### Components

#### 1. **CIAP Model (Contrastive Image-Audio Pretraining)**

The CIAP model consists of two encoders that learn to map images and audio into a shared 512-dimensional embedding space:

- **Image Encoder**: 
  - Backbone: ResNet18 (pretrained on ImageNet)
  - Architecture: ResNet18 features â†’ Linear projection â†’ LayerNorm
  - Input: RGB images (224Ã—224)
  - Output: 512-dimensional normalized embeddings

- **Audio Encoder**:
  - Architecture: MLP with two linear layers
  - Input: Audio waveforms (16kHz, 1 second segments)
  - Output: 512-dimensional normalized embeddings

- **Contrastive Learning**:
  - Uses InfoNCE loss (similar to CLIP/CLAP)
  - Learns to maximize similarity between paired image-audio embeddings
  - Minimizes similarity between unpaired samples
  - Temperature-scaled cosine similarity

#### 2. **Integration with AudioLDM**

- Replaces the CLAP text encoder with CIAP image encoder
- The image embeddings serve as conditioning for the frozen AudioLDM diffusion model
- Zero-shot inference: Generate audio directly from images without fine-tuning the diffusion model

## ğŸ“ Project Structure

```
AudioLDM/
â”œâ”€â”€ audioldm/
â”‚   â”œâ”€â”€ ciap/                    # CIAP model implementation
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ image_encoder.py      # ResNet18-based image encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_encoder.py      # MLP-based audio encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ ciap_cond.py          # Conditioning stage wrapper
â”‚   â”‚   â”‚   â””â”€â”€ ciap_clap_model.py    # CLAP-style wrapper
â”‚   â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”‚   â””â”€â”€ paired_image_audio_dataset.py  # Dataset loader
â”‚   â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”‚   â””â”€â”€ contrastive_loss.py           # Contrastive loss
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”‚   â”œâ”€â”€ train_contrastive.py          # Training script
â”‚   â”‚   â”‚   â””â”€â”€ train_clap_style.py          # Alternative training
â”‚   â”‚   â””â”€â”€ configs/
â”‚   â”‚       â””â”€â”€ ciap_config.yaml              # Configuration file
â”‚   â””â”€â”€ ...                      # AudioLDM baseline (frozen)
â”œâ”€â”€ ckpt/                        # Model checkpoints
â”‚   â”œâ”€â”€ audioldm-s-full.ckpt              # Frozen AudioLDM baseline
â”‚   â”œâ”€â”€ ciap_image_encoder_epoch110.pt    # Trained image encoder
â”‚   â””â”€â”€ ciap_audio_encoder2.pt            # Trained audio encoder
â”œâ”€â”€ data/                        # Training/validation datasets
â”‚   â”œâ”€â”€ train/                   # Paired image-audio training data
â”‚   â””â”€â”€ val/                     # Paired image-audio validation data
â”œâ”€â”€ image_to_audio_ui.py        # Gradio web interface
â””â”€â”€ Inference.ipynb             # Inference notebook
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch (CPU or GPU)
- Required packages:
  ```bash
  pip install torch torchvision torchaudio
  pip install gradio pillow numpy soundfile yaml
  pip install matplotlib tqdm
  ```

### Running the Web Interface

1. **Navigate to the AudioLDM directory:**
   ```bash
   cd AudioLDM
   ```

2. **Launch the Gradio UI:**
   ```bash
   python image_to_audio_ui.py
   ```

3. **Access the interface:**
   - Open your browser and navigate to `http://localhost:7860`
   - The UI will automatically load the models on first use

4. **Generate audio from images:**
   - Upload an image (JPG, PNG, or other formats)
   - Adjust the duration slider (2.5-10 seconds)
   - Click "Generate Audio ğŸµ"
   - Wait for generation to complete (progress bar will show status)
   - Play the generated audio directly in the browser

### Features of the UI

- âœ… **Automatic Image Preprocessing**: Handles various image formats and dimensions
- âœ… **Real-time Progress Tracking**: Shows generation progress with status updates
- âœ… **Audio Playback**: Built-in audio player in the browser
- âœ… **Flexible Duration**: Adjustable audio length (2.5-10 seconds)

## ğŸ”¬ Technical Details

### Training Process

1. **Dataset Preparation**:
   - Collect paired image-audio data
   - Each sample contains an image and corresponding audio file
   - Images are preprocessed to 224Ã—224 RGB
   - Audio is segmented to 1-second clips at 16kHz

2. **Contrastive Learning**:
   - Train image and audio encoders jointly
   - Use InfoNCE contrastive loss
   - Maximize similarity for positive pairs (matched image-audio)
   - Minimize similarity for negative pairs (random combinations)

3. **Training Configuration**:
   - Batch size: 32
   - Learning rate: 0.001
   - Optimizer: Adam
   - Scheduler: StepLR (decay every 10 epochs)

### Inference Process

1. **Image Encoding**:
   - Preprocess image: Resize to 224Ã—224, convert to tensor
   - Pass through Image Encoder â†’ 512-d embedding

2. **Audio Generation**:
   - Use image embedding as condition for AudioLDM
   - Run diffusion sampling (1000 steps)
   - Decode latent to mel-spectrogram
   - Convert mel-spectrogram to waveform

3. **Post-processing**:
   - Crop to desired duration
   - Save as WAV file

## ğŸ“Š Model Files

### Checkpoints

- **AudioLDM Baseline** (`ckpt/audioldm-s-full.ckpt`): 
  - Frozen pretrained diffusion model
  - Used for audio generation only

- **CIAP Encoders**:
  - `ckpt/ciap_image_encoder_epoch110.pt`: Trained image encoder
  - `ckpt/ciap_audio_encoder2.pt`: Trained audio encoder

### Configuration

- Model configuration: `audioldm/ciap/configs/ciap_config.yaml`
- Image encoder: ResNet18 â†’ 512-d projection
- Audio encoder: MLP (16000 â†’ 1024 â†’ 512)

## ğŸ” Key Innovation

The main contribution is replacing the text-based conditioning (CLAP) with image-based conditioning (CIAP) while keeping the AudioLDM diffusion model completely frozen. This demonstrates:

1. **Modular Design**: The conditioning mechanism is separable from the generative model
2. **Zero-shot Transfer**: The pretrained diffusion model can work with new conditioning types without retraining
3. **Efficient Training**: Only the encoders need training, reducing computational cost

## ğŸ“ Usage Example

### Python API

```python
from audioldm import build_model, save_wave
from audioldm.ciap.models.image_encoder import ImageEncoder
from audioldm.ciap.models.audio_encoder import AudioEncoder
from audioldm.ciap.models.ciap_cond import CIAPCondStage
from PIL import Image
import torchvision.transforms as T

# Load models
audioldm = build_model(ckpt_path="./ckpt/audioldm-s-full.ckpt")
image_encoder = ImageEncoder(config).load_state_dict(torch.load("ckpt/ciap_image_encoder_epoch110.pt"))
audio_encoder = AudioEncoder(config).load_state_dict(torch.load("ckpt/ciap_audio_encoder2.pt"))
cond = CIAPCondStage(image_encoder, audio_encoder, embed_dim=512, device="cuda")

# Preprocess image
transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])
image = transform(Image.open("your_image.jpg")).unsqueeze(0).to("cuda")

# Encode and generate
with torch.no_grad():
    img_emb = cond.encode(image)
    waveform_latent = audioldm.sample(cond=img_emb, batch_size=1)
    mel = audioldm.decode_first_stage(waveform_latent)
    waveform = audioldm.mel_spectrogram_to_waveform(mel)

# Save audio
save_wave(waveform, savepath="./output", name="generated_audio")
```

## ğŸ¯ Project Compliance

This project adheres to all specified requirements:

- âœ… Uses AudioLDM as the instructed baseline model
- âœ… Baseline network (AudioLDM diffusion model) remains completely frozen
- âœ… Only CIAP components (image and audio encoders) are trained
- âœ… No modification to the AudioLDM architecture or weights
- âœ… Dataset collected and verified with TAs
- âœ… Demonstrates image-to-audio generation capability

## ğŸ› ï¸ Development

### Training CIAP Models

To train the CIAP encoders:

```bash
cd AudioLDM
python audioldm/ciap/training/train_contrastive.py --config audioldm/ciap/configs/ciap_config.yaml
```

### Evaluation

Use the Jupyter notebook `Inference.ipynb` for detailed inference and evaluation.

## ğŸ“š References

- **AudioLDM**: [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://arxiv.org/abs/2301.12503)
- **CLAP**: Contrastive Language-Audio Pretraining (used as reference architecture)
- **CLIP**: Contrastive Language-Image Pretraining (inspiration for contrastive learning approach)

## ğŸ‘¥ Acknowledgments

- AudioLDM baseline model and codebase
- Dataset contributors and TAs for verification

---

**Note**: This project is for academic/research purposes. The AudioLDM baseline remains unchanged and frozen as per project requirements.

