{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ac09a7",
   "metadata": {},
   "source": [
    "# Training CIAP Contrastive Model on Google Colab\n",
    "This notebook demonstrates how to train the CIAP contrastive model on Google Colab. The trained weights for the ImageEncoder and AudioEncoder models will be saved for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e377249",
   "metadata": {},
   "source": [
    "## Section 1: Install Required Libraries\n",
    "Install necessary libraries such as PyTorch, PyYAML, and tqdm using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install pyyaml tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d8e8b",
   "metadata": {},
   "source": [
    "## Section 2: Clone the Repository and Set Up Environment\n",
    "Clone the AudioLDM repository, navigate to the required directory, and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the AudioLDM repository\n",
    "!git clone https://github.com/haoheliu/AudioLDM.git\n",
    "%cd AudioLDM\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87d468",
   "metadata": {},
   "source": [
    "## Section 3: Load Configuration\n",
    "Load the configuration file (ciap_config.yaml) using PyYAML and parse the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load the configuration file\n",
    "config_path = \"audioldm/ciap/configs/ciap_config.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Display the configuration\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d344ab",
   "metadata": {},
   "source": [
    "## Section 4: Define Dataset and DataLoader\n",
    "Use the `PairedImageAudioDataset` class to define the dataset and create a DataLoader for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a188094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
    "\n",
    "# Extract dataset parameters from the configuration\n",
    "dataset_cfg = config.get(\"dataset\", {})\n",
    "dataset_path = dataset_cfg.get(\"path\", \"\")\n",
    "image_ext = dataset_cfg.get(\"image_extension\", \".png\")\n",
    "audio_ext = dataset_cfg.get(\"audio_extension\", \".wav\")\n",
    "\n",
    "# Define the dataset and DataLoader\n",
    "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
    "batch_size = config[\"training\"].get(\"batch_size\", 32)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ed108",
   "metadata": {},
   "source": [
    "## Section 5: Initialize Models\n",
    "Initialize the `ImageEncoder` and `AudioEncoder` models using the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audioldm.ciap.models.image_encoder import ImageEncoder\n",
    "from audioldm.ciap.models.audio_encoder import AudioEncoder\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device(config.get(\"device\", \"cpu\"))\n",
    "image_encoder = ImageEncoder(config[\"model\"][\"image_encoder\"]).to(device)\n",
    "audio_encoder = AudioEncoder(config[\"model\"][\"audio_encoder\"]).to(device)\n",
    "\n",
    "# Set models to training mode\n",
    "image_encoder.train()\n",
    "audio_encoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38290c47",
   "metadata": {},
   "source": [
    "## Section 6: Define Loss Function and Optimizer\n",
    "Set up the `ContrastiveLoss` function and Adam optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from audioldm.ciap.losses.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = ContrastiveLoss()\n",
    "lr = config[\"training\"].get(\"learning_rate\", 1e-3)\n",
    "optimizer = optim.Adam(list(image_encoder.parameters()) + list(audio_encoder.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57579b",
   "metadata": {},
   "source": [
    "## Section 7: Training Loop\n",
    "Implement the training loop with tqdm for progress tracking, and calculate the loss for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = config[\"training\"].get(\"num_epochs\", 10)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader),\n",
    "                desc=f\"Epoch {epoch}/{num_epochs}\", leave=True)\n",
    "    for batch_idx, (images, audios) in loop:\n",
    "        images = images.to(device)\n",
    "        audios = audios.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use encode() if available; fallback to forward()\n",
    "        img_emb = image_encoder.encode(images) if hasattr(image_encoder, \"encode\") else image_encoder(images)\n",
    "        aud_emb = audio_encoder.encode(audios) if hasattr(audio_encoder, \"encode\") else audio_encoder(audios)\n",
    "\n",
    "        # Labels: i-th image matches i-th audio\n",
    "        labels = torch.arange(img_emb.size(0), device=device)\n",
    "\n",
    "        loss = criterion(img_emb, aud_emb, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        avg_loss = running_loss / batch_idx\n",
    "        loop.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\", \"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch} finished. EpochLoss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25008d9",
   "metadata": {},
   "source": [
    "## Section 8: Save Trained Weights\n",
    "Save the trained weights of the `ImageEncoder` and `AudioEncoder` models to the Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save trained weights\n",
    "output_dir = \"ciap_trained_weights\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "img_path = os.path.join(output_dir, \"ciap_image_encoder.pt\")\n",
    "aud_path = os.path.join(output_dir, \"ciap_audio_encoder.pt\")\n",
    "torch.save(image_encoder.state_dict(), img_path)\n",
    "torch.save(audio_encoder.state_dict(), aud_path)\n",
    "\n",
    "print(f\"Saved image encoder -> {img_path}\")\n",
    "print(f\"Saved audio encoder  -> {aud_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
