{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6ac09a7",
      "metadata": {
        "id": "e6ac09a7"
      },
      "source": [
        "# Training CIAP Contrastive Model on Google Colab\n",
        "This notebook demonstrates how to train the CIAP contrastive model on Google Colab. The trained weights for the ImageEncoder and AudioEncoder models will be saved for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e377249",
      "metadata": {
        "id": "4e377249"
      },
      "source": [
        "## Section 1: Install Required Libraries\n",
        "Install necessary libraries such as PyTorch, PyYAML, and tqdm using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f06fdc3",
      "metadata": {
        "id": "9f06fdc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0689e4d-27a3-41ac-b26f-73dad6d505a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12065 sha256=ff21b40eb8b8461ed971d4c0e9a9bb2f27b6286c3b862720535d300db5da6971\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/4d/c7/f3cf0f75c746c219090060131fe00f1523cc2c5484991f4030\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar\n",
            "Successfully installed progressbar-2.5\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting torchlibrosa\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchlibrosa) (2.0.2)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchlibrosa) (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa>=0.8.0->torchlibrosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.8.0->torchlibrosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (2025.10.5)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa\n",
            "Successfully installed torchlibrosa-0.1.0\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pyyaml tqdm\n",
        "!pip install progressbar\n",
        "!pip install ftfy\n",
        "!pip install torchlibrosa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09d8e8b",
      "metadata": {
        "id": "a09d8e8b"
      },
      "source": [
        "## Section 2: Clone the Repository and Set Up Environment\n",
        "Clone the AudioLDM repository, navigate to the required directory, and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0b0c5ccd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b0c5ccd",
        "outputId": "3cc3b76f-5c00-4901-ae12-7f84cb607293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IE-643'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 172 (delta 43), reused 168 (delta 39), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (172/172), 1.67 MiB | 6.05 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/IE-643/AudioLDM\n"
          ]
        }
      ],
      "source": [
        "# Remove existing repository if it exists and clone again\n",
        "!rm -rf IE-643\n",
        "!git clone https://github.com/luffy-taro-106/IE-643.git\n",
        "%cd IE-643/AudioLDM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a87d468",
      "metadata": {
        "id": "4a87d468"
      },
      "source": [
        "## Section 3: Load Configuration\n",
        "Load the configuration file (ciap_config.yaml) using PyYAML and parse the training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1afccc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1afccc6",
        "outputId": "e05a21e7-0867-49d1-f104-2b3ff32bb775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': {'type': 'ContrastiveModel', 'image_encoder': {'type': 'ImageEncoder', 'input_size': [3, 224, 224], 'output_size': 512, 'pretrained': True}, 'audio_encoder': {'type': 'AudioEncoder', 'input_size': [1, 16000], 'output_size': 512, 'pretrained': True}}, 'training': {'batch_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'weight_decay': '1e-5', 'scheduler': {'type': 'StepLR', 'step_size': 10, 'gamma': 0.1}}, 'dataset': {'train': {'path': 'data/train', 'image_extension': '.jpg', 'audio_extension': '.wav'}, 'val': {'path': 'data/val', 'image_extension': '.jpg', 'audio_extension': '.wav'}}, 'logging': {'log_dir': 'logs', 'log_interval': 10}, 'device': {'type': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "# Load the configuration file\n",
        "config_path = \"audioldm/ciap/configs/ciap_config.yaml\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "# Display the configuration\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d344ab",
      "metadata": {
        "id": "71d344ab"
      },
      "source": [
        "## Section 4: Define Dataset and DataLoader\n",
        "Use the `PairedImageAudioDataset` class to define the dataset and create a DataLoader for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8a188094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "8a188094",
        "outputId": "a66e5193-1c36-494a-acdd-8617cf94705a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3194227450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPairedImageAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataloader created with {len(dataloader)} batches.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
        "import os\n",
        "\n",
        "# Extract dataset parameters from the configuration\n",
        "dataset_cfg = config.get(\"dataset\", {})\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset path - **Please verify this path and ensure it contains image and audio files**\n",
        "dataset_path = \"/content/drive/My Drive/data/train\"\n",
        "\n",
        "\n",
        "image_ext = dataset_cfg.get(\"image_extension\", \".jpg\")\n",
        "audio_ext = dataset_cfg.get(\"audio_extension\", \".wav\")\n",
        "\n",
        "    # Define the dataset and DataLoader\n",
        "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
        "batch_size = config[\"training\"].get(\"batch_size\", 32)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
        "print(f\"Dataloader created with {len(dataloader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353ed108",
      "metadata": {
        "id": "353ed108"
      },
      "source": [
        "## Section 5: Initialize Models\n",
        "Initialize the `ImageEncoder` and `AudioEncoder` models using the configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0b0092",
      "metadata": {
        "id": "8e0b0092"
      },
      "outputs": [],
      "source": [
        "# ---------- New: Model init + training loop using CLAP audio (HTSAT/PANN) + ResNet image encoder ----------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use the CIAP CLAP-style model (wraps clap audio model + your ResNet image encoder)\n",
        "from audioldm.ciap.models.ciap_clap_model import CIAP_CLAP_Model\n",
        "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
        "\n",
        "# Simple contrastive loss (InfoNCE-style) — reuse your notebook's implementation if present\n",
        "import torch.nn.functional as F\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "        audio_embeddings = F.normalize(audio_embeddings, p=2, dim=-1)\n",
        "        logits = torch.matmul(image_embeddings, audio_embeddings.T) / self.temperature\n",
        "        labels = torch.arange(image_embeddings.size(0), device=image_embeddings.device)\n",
        "        loss_i2a = F.cross_entropy(logits, labels)\n",
        "        loss_a2i = F.cross_entropy(logits.T, labels)\n",
        "        return 0.5 * (loss_i2a + loss_a2i)\n",
        "\n",
        "# Load config values if available\n",
        "cfg = config if \"config\" in globals() else {}\n",
        "dataset_cfg = cfg.get(\"dataset\", {})\n",
        "training_cfg = cfg.get(\"training\", {})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# Model: use CLAP audio model (HTSAT-tiny by default) and ResNet image encoder\n",
        "amodel = training_cfg.get(\"amodel\", \"HTSAT-tiny\")\n",
        "embed_dim = int(training_cfg.get(\"embed_dim\", 512))\n",
        "pretrained_audio_ckpt = training_cfg.get(\"pretrained_audio_ckpt\", \"\")\n",
        "\n",
        "model = CIAP_CLAP_Model(amodel=amodel, tmodel=\"roberta\", pretrained_path=pretrained_audio_ckpt, image_proj_dim=embed_dim, device=device)\n",
        "model.to(device)\n",
        "model.train()  # image encoder will be trained\n",
        "\n",
        "# Optimizer: train image encoder params (and projection head if you add one)\n",
        "trainable_params = list(model.image_encoder.parameters())\n",
        "optimizer = optim.Adam(trainable_params, lr=float(training_cfg.get(\"learning_rate\", 1e-4)), weight_decay=1e-6)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(training_cfg.get(\"lr_step\", 10)), gamma=float(training_cfg.get(\"lr_gamma\", 0.5)))\n",
        "\n",
        "criterion = ContrastiveLoss(temperature=float(training_cfg.get(\"temperature\", 0.07)))\n",
        "\n",
        "# Training loop\n",
        "epochs = int(training_cfg.get(\"epochs\", 150))\n",
        "save_dir = training_cfg.get(\"save_dir\", \"./ckpt\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Inject print statements into the cloned get_audio_features function\n",
        "from audioldm.clap.training.data import get_audio_features as original_get_audio_features\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def get_audio_features(sample, audio_data, max_len_unused, data_truncating, data_filling, audio_cfg):\n",
        "    # Use a fixed reduced max_len for repetition (5s @ 48k)\n",
        "    max_len = 48000 * 5\n",
        "    # audio_data may be torch.Tensor with shape [1, T] or [T]; make 1D waveform\n",
        "    if isinstance(audio_data, torch.Tensor):\n",
        "        a = audio_data.squeeze()\n",
        "        if a.dim() > 1:\n",
        "            a = a.view(-1)\n",
        "    else:\n",
        "        a = audio_data\n",
        "\n",
        "    if data_filling == \"repeatpad\":\n",
        "        cur_len = a.shape[-1] if isinstance(a, torch.Tensor) else len(a)\n",
        "        if cur_len == 0:\n",
        "            n_repeat = 0\n",
        "        else:\n",
        "            n_repeat = int(math.ceil(max_len / cur_len))\n",
        "        if n_repeat > 0:\n",
        "            a = a.repeat(n_repeat)\n",
        "\n",
        "    # call original with corrected 1D audio tensor and new max_len\n",
        "    return original_get_audio_features(sample, a, max_len, data_truncating, data_filling, audio_cfg)\n",
        "\n",
        "# monkey patch\n",
        "model.preprocess_audio_waveform.__globals__['get_audio_features'] = get_audio_features\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader), desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "    for batch_idx, (images, audios, *rest) in loop:\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prepare audio dicts expected by CLAP audio model\n",
        "        # The max_len will now be controlled within the monkey-patched get_audio_features\n",
        "        audio_dicts = model.preprocess_audio_waveform(audios)\n",
        "        aud_emb = model.get_audio_embedding(audio_dicts)      # [B, D]\n",
        "        img_emb = model.get_image_embedding(images)           # [B, D]\n",
        "\n",
        "        # normalize embeddings\n",
        "        aud_emb = aud_emb / (aud_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "        img_emb = img_emb / (img_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        loss = criterion(img_emb, aud_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        avg_loss = running_loss / batch_idx\n",
        "        loop.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\", \"avg_loss\": f\"{avg_loss:.4f}\"})\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch} finished. Avg loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # save image encoder checkpoint each epoch\n",
        "    img_ckpt = os.path.join(save_dir, f\"ciap_image_encoder_epoch{epoch}.pt\")\n",
        "    torch.save(model.image_encoder.state_dict(), img_ckpt)\n",
        "    print(\"Saved image encoder to\", img_ckpt)\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25008d9",
      "metadata": {
        "id": "e25008d9"
      },
      "source": [
        "## Section 8: Save Trained Weights\n",
        "Save the trained weights of the `ImageEncoder` and `AudioEncoder` models to the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11033d3c",
      "metadata": {
        "id": "11033d3c"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_dir = \"ckpt\" # Updated to the correct source directory\n",
        "destination_dir = \"/content/drive/My Drive/ciap_trained_weights3\" # Replace with your desired path in Google Drive\n",
        "\n",
        "# Create the destination directory in Google Drive if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy the saved weights to Google Drive\n",
        "try:\n",
        "    shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)\n",
        "    print(f\"Successfully copied trained weights from {source_dir} to {destination_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying files: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94a4527a"
      },
      "source": [
        "# Code to resume training from a checkpoint\n",
        "\n",
        "# Define the epoch you want to resume from\n",
        "resume_epoch = 50 # Replace with the desired epoch number\n",
        "\n",
        "# Define the path to the saved image encoder checkpoint\n",
        "# Make sure this path is correct and accessible (e.g., in your Google Drive)\n",
        "checkpoint_path = f\"/content/drive/My Drive/ciap_trained_weights3/ciap_image_encoder_epoch{resume_epoch}.pt\"\n",
        "\n",
        "# Load the saved state dictionary\n",
        "try:\n",
        "    model.image_encoder.load_state_dict(torch.load(checkpoint_path))\n",
        "    print(f\"Successfully loaded image encoder weights from {checkpoint_path}\")\n",
        "\n",
        "    # You might also need to load optimizer and scheduler states if you saved them\n",
        "    # For example:\n",
        "    # optimizer_checkpoint_path = f\"./ckpt/ciap_optimizer_epoch{resume_epoch}.pt\"\n",
        "    # optimizer.load_state_dict(torch.load(optimizer_checkpoint_path))\n",
        "    # scheduler_checkpoint_path = f\"./ckpt/ciap_scheduler_epoch{resume_epoch}.pt\"\n",
        "    # scheduler.load_state_dict(torch.load(scheduler_checkpoint_path))\n",
        "\n",
        "\n",
        "    # Continue training from the next epoch\n",
        "    for epoch in range(resume_epoch + 1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader), desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "        for batch_idx, (images, audios, *rest) in loop:\n",
        "            images = images.to(device)\n",
        "            audios = audios.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            audio_dicts = model.preprocess_audio_waveform(audios)\n",
        "            aud_emb = model.get_audio_embedding(audio_dicts)      # [B, D]\n",
        "            img_emb = model.get_image_embedding(images)           # [B, D]\n",
        "\n",
        "            aud_emb = aud_emb / (aud_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "            img_emb = img_emb / (img_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "            loss = criterion(img_emb, aud_emb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            avg_loss = running_loss / batch_idx\n",
        "            loop.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\", \"avg_loss\": f\"{avg_loss:.4f}\"})\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch} finished. Avg loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "        # save image encoder checkpoint each epoch\n",
        "        img_ckpt = os.path.join(save_dir, f\"ciap_image_encoder_epoch{epoch}.pt\")\n",
        "        torch.save(model.image_encoder.state_dict(), img_ckpt)\n",
        "        print(\"Saved image encoder to\", img_ckpt)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Checkpoint file not found at {checkpoint_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "id": "94a4527a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}