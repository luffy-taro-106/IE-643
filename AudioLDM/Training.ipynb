{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6ac09a7",
      "metadata": {
        "id": "e6ac09a7"
      },
      "source": [
        "# Training CIAP Contrastive Model on Google Colab\n",
        "This notebook demonstrates how to train the CIAP contrastive model on Google Colab. The trained weights for the ImageEncoder and AudioEncoder models will be saved for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e377249",
      "metadata": {
        "id": "4e377249"
      },
      "source": [
        "## Section 1: Install Required Libraries\n",
        "Install necessary libraries such as PyTorch, PyYAML, and tqdm using pip."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09d8e8b",
      "metadata": {
        "id": "a09d8e8b"
      },
      "source": [
        "## Section 2: Clone the Repository and Set Up Environment\n",
        "Clone the AudioLDM repository, navigate to the required directory, and install dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a87d468",
      "metadata": {
        "id": "4a87d468"
      },
      "source": [
        "## Section 3: Load Configuration\n",
        "Load the configuration file (ciap_config.yaml) using PyYAML and parse the training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1afccc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1afccc6",
        "outputId": "4b41e9cd-c735-4d96-a7fd-aa1ee9df1434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': {'type': 'ContrastiveModel', 'image_encoder': {'type': 'ImageEncoder', 'input_size': [3, 224, 224], 'output_size': 512, 'pretrained': True}, 'audio_encoder': {'type': 'AudioEncoder', 'input_size': [1, 16000], 'output_size': 512, 'pretrained': True}}, 'training': {'batch_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'weight_decay': '1e-5', 'scheduler': {'type': 'StepLR', 'step_size': 10, 'gamma': 0.1}}, 'dataset': {'train': {'path': 'data/train', 'image_extension': '.jpg', 'audio_extension': '.wav'}, 'val': {'path': 'data/val', 'image_extension': '.jpg', 'audio_extension': '.wav'}}, 'logging': {'log_dir': 'logs', 'log_interval': 10}, 'device': {'type': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "# Load the configuration file\n",
        "config_path = \"audioldm/ciap/configs/ciap_config.yaml\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "# Display the configuration\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d344ab",
      "metadata": {
        "id": "71d344ab"
      },
      "source": [
        "## Section 4: Define Dataset and DataLoader\n",
        "Use the `PairedImageAudioDataset` class to define the dataset and create a DataLoader for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a188094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a188094",
        "outputId": "0825a2ec-ca11-443c-f147-f72554e2e7fa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
        "\n",
        "# Extract dataset parameters from the configuration\n",
        "dataset_cfg = config.get(\"dataset\", {})\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Set dataset path - **Please verify this path and ensure it contains image and audio files**\n",
        "# dataset_path = \"/content/drive/My Drive/data/train\"\n",
        "\n",
        "dataset_path = dataset_cfg.get(\"dataset_path\", \"./data/train\")\n",
        "image_ext = dataset_cfg.get(\"image_extension\", \".jpg\")\n",
        "audio_ext = dataset_cfg.get(\"audio_extension\", \".wav\")\n",
        "\n",
        "# Define the dataset and DataLoader\n",
        "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
        "batch_size = config[\"training\"].get(\"batch_size\", 32)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353ed108",
      "metadata": {
        "id": "353ed108"
      },
      "source": [
        "## Section 5: Initialize Models\n",
        "Initialize the `ImageEncoder` and `AudioEncoder` models using the configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b856327c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b856327c",
        "outputId": "dc0a6567-d9db-4aee-9c3a-f06ead1ad6ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AudioEncoder(\n",
              "  (fc1): Linear(in_features=16000, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from audioldm.ciap.models.image_encoder import ImageEncoder\n",
        "from audioldm.ciap.models.audio_encoder import AudioEncoder\n",
        "\n",
        "# Initialize models\n",
        "device_config = config.get(\"device\", {\"type\": \"cpu\"})\n",
        "# Check if CUDA is available and use GPU if it is, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_encoder = ImageEncoder(config[\"model\"][\"image_encoder\"]).to(device)\n",
        "audio_encoder = AudioEncoder(config[\"model\"][\"audio_encoder\"]).to(device)\n",
        "\n",
        "# Set models to training mode\n",
        "image_encoder.train()\n",
        "audio_encoder.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38290c47",
      "metadata": {
        "id": "38290c47"
      },
      "source": [
        "## Section 6: Define Loss Function and Optimizer\n",
        "Set up the `ContrastiveLoss` function and Adam optimizer for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8f06771c",
      "metadata": {
        "id": "8f06771c"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from audioldm.ciap.losses.contrastive_loss import ContrastiveLoss\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = ContrastiveLoss()\n",
        "lr = config[\"training\"].get(\"learning_rate\", 1e-3)\n",
        "optimizer = optim.Adam(list(image_encoder.parameters()) + list(audio_encoder.parameters()), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc57579b",
      "metadata": {
        "id": "fc57579b"
      },
      "source": [
        "## Section 7: Training Loop\n",
        "Implement the training loop with tqdm for progress tracking, and calculate the loss for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "face4250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "face4250",
        "outputId": "61aa98d1-413f-471d-a521-06eb277ea23a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/luffy_sama/Desktop/Workspace/IE_643/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/50:  21%|██▏       | 27/127 [00:36<02:11,  1.32s/it, avg_loss=3.6003, batch_loss=3.4638]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = config[\"training\"].get(\"num_epochs\", 10)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader),\n",
        "                desc=f\"Epoch {epoch}/{num_epochs}\", leave=True)\n",
        "    for batch_idx, (images, audios) in loop:\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use encode() if available; fallback to forward()\n",
        "        img_emb = image_encoder.encode(images) if hasattr(image_encoder, \"encode\") else image_encoder(images)\n",
        "        aud_emb = audio_encoder.encode(audios) if hasattr(audio_encoder, \"encode\") else audio_encoder(audios)\n",
        "\n",
        "        # Labels: i-th image matches i-th audio\n",
        "\n",
        "\n",
        "        loss = criterion(img_emb, aud_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        avg_loss = running_loss / batch_idx\n",
        "        loop.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\", \"batch_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    print(f\"Epoch {epoch} finished. EpochLoss: {running_loss/len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25008d9",
      "metadata": {
        "id": "e25008d9"
      },
      "source": [
        "## Section 8: Save Trained Weights\n",
        "Save the trained weights of the `ImageEncoder` and `AudioEncoder` models to the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8064818c",
      "metadata": {
        "id": "8064818c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Save trained weights\n",
        "output_dir = \"ciap_trained_weights\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "img_path = os.path.join(output_dir, \"ciap_image_encoder.pt\")\n",
        "aud_path = os.path.join(output_dir, \"ciap_audio_encoder.pt\")\n",
        "torch.save(image_encoder.state_dict(), img_path)\n",
        "torch.save(audio_encoder.state_dict(), aud_path)\n",
        "\n",
        "print(f\"Saved image encoder -> {img_path}\")\n",
        "print(f\"Saved audio encoder  -> {aud_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
