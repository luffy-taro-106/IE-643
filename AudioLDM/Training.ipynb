{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6ac09a7",
      "metadata": {
        "id": "e6ac09a7"
      },
      "source": [
        "# Training CIAP Contrastive Model on Google Colab\n",
        "This notebook demonstrates how to train the CIAP contrastive model on Google Colab. The trained weights for the ImageEncoder and AudioEncoder models will be saved for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e377249",
      "metadata": {
        "id": "4e377249"
      },
      "source": [
        "## Section 1: Install Required Libraries\n",
        "Install necessary libraries such as PyTorch, PyYAML, and tqdm using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f06fdc3",
      "metadata": {
        "id": "9f06fdc3"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pyyaml tqdm\n",
        "!pip install progressbar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09d8e8b",
      "metadata": {
        "id": "a09d8e8b"
      },
      "source": [
        "## Section 2: Clone the Repository and Set Up Environment\n",
        "Clone the AudioLDM repository, navigate to the required directory, and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0c5ccd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b0c5ccd",
        "outputId": "3caab27d-f022-4330-efe5-8af580a0ca45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'IE-643'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 148 (delta 29), reused 147 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (148/148), 1.47 MiB | 29.44 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/IE-643/AudioLDM\n"
          ]
        }
      ],
      "source": [
        "# Remove existing repository if it exists and clone again\n",
        "!rm -rf IE-643\n",
        "!git clone https://github.com/luffy-taro-106/IE-643.git\n",
        "%cd IE-643/AudioLDM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a87d468",
      "metadata": {
        "id": "4a87d468"
      },
      "source": [
        "## Section 3: Load Configuration\n",
        "Load the configuration file (ciap_config.yaml) using PyYAML and parse the training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1afccc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1afccc6",
        "outputId": "30fa34b4-8de8-4085-b8fa-8b8408f6253e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model': {'type': 'ContrastiveModel', 'image_encoder': {'type': 'ImageEncoder', 'input_size': [3, 224, 224], 'output_size': 512, 'pretrained': True}, 'audio_encoder': {'type': 'AudioEncoder', 'input_size': [1, 16000], 'output_size': 512, 'pretrained': True}}, 'training': {'batch_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'weight_decay': '1e-5', 'scheduler': {'type': 'StepLR', 'step_size': 10, 'gamma': 0.1}}, 'dataset': {'train': {'path': 'data/train', 'image_extension': '.jpg', 'audio_extension': '.wav'}, 'val': {'path': 'data/val', 'image_extension': '.jpg', 'audio_extension': '.wav'}}, 'logging': {'log_dir': 'logs', 'log_interval': 10}, 'device': {'type': 'cuda'}}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "# Load the configuration file\n",
        "config_path = \"audioldm/ciap/configs/ciap_config.yaml\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "# Display the configuration\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d344ab",
      "metadata": {
        "id": "71d344ab"
      },
      "source": [
        "## Section 4: Define Dataset and DataLoader\n",
        "Use the `PairedImageAudioDataset` class to define the dataset and create a DataLoader for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a188094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a188094",
        "outputId": "83baeb4c-8e35-4062-87e7-65ff5fc519d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataloader created with 127 batches.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
        "import os\n",
        "\n",
        "# Extract dataset parameters from the configuration\n",
        "dataset_cfg = config.get(\"dataset\", {})\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset path - **Please verify this path and ensure it contains image and audio files**\n",
        "dataset_path = \"/content/drive/My Drive/data/train\"\n",
        "\n",
        "\n",
        "image_ext = dataset_cfg.get(\"image_extension\", \".jpg\")\n",
        "audio_ext = dataset_cfg.get(\"audio_extension\", \".wav\")\n",
        "\n",
        "    # Define the dataset and DataLoader\n",
        "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
        "batch_size = config[\"training\"].get(\"batch_size\", 32)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
        "print(f\"Dataloader created with {len(dataloader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353ed108",
      "metadata": {
        "id": "353ed108"
      },
      "source": [
        "## Section 5: Initialize Models\n",
        "Initialize the `ImageEncoder` and `AudioEncoder` models using the configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0b0092",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ...existing code...\n",
        "\n",
        "# ---------- New: Model init + training loop using CLAP audio (HTSAT/PANN) + ResNet image encoder ----------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use the CIAP CLAP-style model (wraps clap audio model + your ResNet image encoder)\n",
        "from audioldm.ciap.models.ciap_clap_model import CIAP_CLAP_Model\n",
        "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
        "\n",
        "# Simple contrastive loss (InfoNCE-style) â€” reuse your notebook's implementation if present\n",
        "import torch.nn.functional as F\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "        audio_embeddings = F.normalize(audio_embeddings, p=2, dim=-1)\n",
        "        logits = torch.matmul(image_embeddings, audio_embeddings.T) / self.temperature\n",
        "        labels = torch.arange(image_embeddings.size(0), device=image_embeddings.device)\n",
        "        loss_i2a = F.cross_entropy(logits, labels)\n",
        "        loss_a2i = F.cross_entropy(logits.T, labels)\n",
        "        return 0.5 * (loss_i2a + loss_a2i)\n",
        "\n",
        "# Load config values if available\n",
        "cfg = config if \"config\" in globals() else {}\n",
        "dataset_cfg = cfg.get(\"dataset\", {})\n",
        "training_cfg = cfg.get(\"training\", {})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Dataset\n",
        "dataset_path = dataset_cfg.get(\"train\", {}).get(\"path\", \"./data/train\")\n",
        "image_ext = dataset_cfg.get(\"image_extension\", \".jpg\")\n",
        "audio_ext = dataset_cfg.get(\"audio_extension\", \".wav\")\n",
        "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
        "batch_size = int(training_cfg.get(\"batch_size\", 16))\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "print(\"Dataset samples:\", len(dataset), \"Batches:\", len(dataloader))\n",
        "\n",
        "# Model: use CLAP audio model (HTSAT-tiny by default) and ResNet image encoder\n",
        "amodel = training_cfg.get(\"amodel\", \"HTSAT-tiny\")\n",
        "embed_dim = int(training_cfg.get(\"embed_dim\", 512))\n",
        "pretrained_audio_ckpt = training_cfg.get(\"pretrained_audio_ckpt\", \"\")\n",
        "\n",
        "model = CIAP_CLAP_Model(amodel=amodel, tmodel=\"roberta\", pretrained_path=pretrained_audio_ckpt, image_proj_dim=embed_dim, device=device)\n",
        "model.to(device)\n",
        "model.train()  # image encoder will be trained\n",
        "\n",
        "# Optimizer: train image encoder params (and projection head if you add one)\n",
        "trainable_params = list(model.image_encoder.parameters())\n",
        "optimizer = optim.Adam(trainable_params, lr=float(training_cfg.get(\"learning_rate\", 1e-4)), weight_decay=1e-6)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(training_cfg.get(\"lr_step\", 10)), gamma=float(training_cfg.get(\"lr_gamma\", 0.5)))\n",
        "\n",
        "criterion = ContrastiveLoss(temperature=float(training_cfg.get(\"temperature\", 0.07)))\n",
        "\n",
        "# Training loop\n",
        "epochs = int(training_cfg.get(\"epochs\", 10))\n",
        "save_dir = training_cfg.get(\"save_dir\", \"./ckpt\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    loop = tqdm(enumerate(dataloader, start=1), total=len(dataloader), desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "    for batch_idx, (images, audios, *rest) in loop:\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # prepare audio dicts expected by CLAP audio model\n",
        "        audio_dicts = model.preprocess_audio_waveform(audios)\n",
        "        aud_emb = model.get_audio_embedding(audio_dicts)      # [B, D]\n",
        "        img_emb = model.get_image_embedding(images)           # [B, D]\n",
        "\n",
        "        # normalize embeddings\n",
        "        aud_emb = aud_emb / (aud_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "        img_emb = img_emb / (img_emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        loss = criterion(img_emb, aud_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        avg_loss = running_loss / batch_idx\n",
        "        loop.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\", \"avg_loss\": f\"{avg_loss:.4f}\"})\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch} finished. Avg loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    # save image encoder checkpoint each epoch\n",
        "    img_ckpt = os.path.join(save_dir, f\"ciap_image_encoder_epoch{epoch}.pt\")\n",
        "    torch.save(model.image_encoder.state_dict(), img_ckpt)\n",
        "    print(\"Saved image encoder to\", img_ckpt)\n",
        "\n",
        "print(\"Training complete.\")\n",
        "# ...existing code..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25008d9",
      "metadata": {
        "id": "e25008d9"
      },
      "source": [
        "## Section 8: Save Trained Weights\n",
        "Save the trained weights of the `ImageEncoder` and `AudioEncoder` models to the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8064818c",
      "metadata": {
        "id": "8064818c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Save trained weights\n",
        "output_dir = \"ciap_trained_weights\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "img_path = os.path.join(output_dir, \"ciap_image_encoder.pt\")\n",
        "aud_path = os.path.join(output_dir, \"ciap_audio_encoder.pt\")\n",
        "torch.save(image_encoder.state_dict(), img_path)\n",
        "torch.save(audio_encoder.state_dict(), aud_path)\n",
        "\n",
        "print(f\"Saved image encoder -> {img_path}\")\n",
        "print(f\"Saved audio encoder  -> {aud_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11033d3c",
      "metadata": {
        "id": "11033d3c"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_dir = \"ciap_trained_weights\"\n",
        "destination_dir = \"/content/drive/My Drive/ciap_trained_weights2\" # Replace with your desired path in Google Drive\n",
        "\n",
        "# Create the destination directory in Google Drive if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy the saved weights to Google Drive\n",
        "try:\n",
        "    shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)\n",
        "    print(f\"Successfully copied trained weights from {source_dir} to {destination_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying files: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zotPFJaSZsXK",
      "metadata": {
        "id": "zotPFJaSZsXK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
