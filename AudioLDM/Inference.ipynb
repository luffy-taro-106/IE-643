{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5627575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config_path = \"audioldm/ciap/configs/ciap_config.yaml\"\n",
    "config = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cf95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audioldm.ciap.models.image_encoder import ImageEncoder\n",
    "from audioldm.ciap.models.audio_encoder import AudioEncoder\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the models\n",
    "image_encoder = ImageEncoder(config[\"model\"][\"image_encoder\"]).to(device)\n",
    "audio_encoder = AudioEncoder(config[\"model\"][\"audio_encoder\"]).to(device)\n",
    "\n",
    "# Load the checkpoints\n",
    "image_encoder_ckpt = \"/Users/luffy_sama/Desktop/Workspace/IE_643/AudioLDM/ckpt/ciap/ciap_image_encoder.pt\"\n",
    "audio_encoder_ckpt = \"/Users/luffy_sama/Desktop/Workspace/IE_643/AudioLDM/ckpt/ciap/ciap_audio_encoder.pt\"\n",
    "\n",
    "image_encoder.load_state_dict(torch.load(image_encoder_ckpt, map_location=device))\n",
    "audio_encoder.load_state_dict(torch.load(audio_encoder_ckpt, map_location=device))\n",
    "\n",
    "print(\"Checkpoints loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e762fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from audioldm import build_model, save_wave\n",
    "from audioldm.ciap.models.image_encoder import ImageEncoder\n",
    "from audioldm.ciap.models.audio_encoder import AudioEncoder\n",
    "from audioldm.ciap.datasets.paired_image_audio_dataset import PairedImageAudioDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set CIAP models to evaluation mode\n",
    "image_encoder.eval()\n",
    "audio_encoder.eval()\n",
    "\n",
    "# Load LDM model\n",
    "ldm_ckpt_path = \"./ckpt/audioldm-s-full.ckpt\"\n",
    "audioldm = build_model(ckpt_path=ldm_ckpt_path)\n",
    "\n",
    "# Define dataset for inference\n",
    "dataset_path = config[\"dataset\"][\"val\"][\"path\"]  # Use the validation dataset path\n",
    "image_ext = config[\"dataset\"][\"val\"].get(\"image_extension\", \".png\")\n",
    "audio_ext = config[\"dataset\"][\"val\"].get(\"audio_extension\", \".wav\")\n",
    "\n",
    "dataset = PairedImageAudioDataset(dataset_path, image_ext=image_ext, audio_ext=audio_ext)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Perform inference\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (image, audio) in enumerate(dataloader):\n",
    "        image = image.to(device)\n",
    "        audio = audio.to(device)\n",
    "\n",
    "        # Step 1: Get embeddings from CIAP model\n",
    "        img_emb = image_encoder.encode(image) if hasattr(image_encoder, \"encode\") else image_encoder(image)\n",
    "        aud_emb = audio_encoder.encode(audio) if hasattr(audio_encoder, \"encode\") else audio_encoder(audio)\n",
    "\n",
    "        print(f\"Sample {idx + 1}:\")\n",
    "        print(f\"Image Embedding: {img_emb}\")\n",
    "        print(f\"Audio Embedding: {aud_emb}\")\n",
    "\n",
    "        # Step 2: Use LDM to generate audio from embeddings\n",
    "        # Here, we use the text prompt as a placeholder for embeddings\n",
    "        text_prompt = f\"Generated audio for sample {idx + 1}\"\n",
    "        waveform = audioldm.generate(\n",
    "            text=text_prompt,\n",
    "            seed=42,\n",
    "            duration=10.0,\n",
    "            guidance_scale=2.5,\n",
    "            n_candidate_gen_per_text=1,\n",
    "            batchsize=1,\n",
    "        )\n",
    "\n",
    "        # Step 3: Save the generated audio\n",
    "        save_wave(waveform, save_path=output_dir, name=f\"generated_audio_{idx + 1}\")\n",
    "        print(f\"Generated audio saved to {output_dir}/generated_audio_{idx + 1}.wav\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
